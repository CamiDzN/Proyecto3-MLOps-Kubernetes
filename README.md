# üß† Descripci√≥n General del Proyecto

Este proyecto implementa una soluci√≥n completa de MLOps distribuida en tres servidores, dise√±ada para gestionar todo el ciclo de vida de un modelo de machine learning que predice la probabilidad de readmisi√≥n hospitalaria de pacientes con diabetes en un periodo de 30 d√≠as.

La arquitectura del proyecto est√° basada en contenedores Docker orquestados con Kubernetes (MicroK8s) y est√° organizada en tres entornos funcionales independientes, desplegados en m√°quinas virtuales diferentes:

- **Servidor 1**: Encargado del preprocesamiento autom√°tico de datos con Apache Airflow.
- **Servidor 2**: Responsable del registro de experimentos y gesti√≥n de artefactos con MLflow y MinIO.
- **Servidor 3**: Despliega el modelo en producci√≥n mediante una API con FastAPI, integra monitoreo con Prometheus & Grafana, pruebas de carga con Locust y una interfaz de usuario con Streamlit.

Este enfoque modular permite escalar y mantener cada componente de forma independiente, emulando un entorno real de producci√≥n distribuido.

> üí° El objetivo principal es optimizar la gesti√≥n hospitalaria mediante un sistema predictivo que permite anticipar la readmisi√≥n de pacientes, mejorando as√≠ la eficiencia de los recursos m√©dicos disponibles.

---

## üóÇÔ∏è Distribuci√≥n del Proyecto por Servidores

Este proyecto fue desarrollado colaborativamente y distribuido en tres m√°quinas virtuales, cada una encargada de un componente clave del flujo de trabajo MLOps. Cada servidor tiene su propio `README.md` con detalles t√©cnicos y operativos espec√≠ficos:

| Servidor | Rol Principal                                   | Enlace al Detalle |
|----------|--------------------------------------------------|-------------------|
| üü¶ Servidor 1 | Preprocesamiento de datos con Airflow           | [Ver README Servidor 1](./Servidor1/README.md) |
| üü© Servidor 2 | Seguimiento de experimentos con MLflow y MinIO  | [Ver README Servidor 2](./Servidor2/README.md) |
| üü• Servidor 3 | Despliegue, monitoreo y pruebas de inferencia   | [Ver README Servidor 3](./Servidor3/README.md) |

Cada una de estas secciones incluye:
- Los contenedores desplegados.
- Los DAGs y notebooks asociados.
- Instrucciones de uso y pruebas.

> üìå **Nota:** Todos los servidores est√°n conectados en red local y comparten el acceso a la base de datos y el almacenamiento distribuido configurado para simular un entorno de producci√≥n real.

---

---

## üß± Arquitectura General del Proyecto

El proyecto est√° distribuido en **tres servidores (m√°quinas virtuales)** que trabajan de manera coordinada para implementar un pipeline completo de MLOps. Cada servidor aloja componentes espec√≠ficos de la arquitectura, asegurando modularidad, escalabilidad y claridad en la implementaci√≥n.

A continuaci√≥n se presenta el diagrama de la arquitectura general:

![Arquitectura del Proyecto](![image](https://github.com/user-attachments/assets/ab94263f-fc2e-488d-b24f-562a5c87e984)) 

### üîπ Servidor 1 ‚Äì Preprocesamiento y Almacenamiento de Datos
- **Airflow**: Orquestaci√≥n de pipelines de preprocesamiento y entrenamiento.
- **Base de Datos MySQL**: Almacena datos en dos capas:
  - `RawData`: Datos crudos separados en train, validation y test.
  - `CleanData`: Datos preprocesados listos para entrenamiento.
- **JupyterLab**: Desarrollo exploratorio y carga de datos desde notebooks.
- **DAGs**:
  - `preprocess_incremental`: Preprocesamiento autom√°tico.
  - `train_and_register`: Entrenamiento y selecci√≥n del mejor modelo.

### üî∏ Servidor 2 ‚Äì Seguimiento de Experimentos
- **MLflow Tracking Server**: Registro de m√©tricas, par√°metros y artefactos.
- **MinIO**: Almacenamiento compatible con S3 para guardar artefactos de modelos.
- **MySQL Metadata**: Almacena la metadata generada por MLflow.
- Imagen personalizada de MLflow desplegada con dependencias para conectividad segura.

### üî∫ Servidor 3 ‚Äì Despliegue, Observabilidad y Experiencia de Usuario
- **FastAPI**: API de inferencia conectada al modelo en producci√≥n desde MLflow.
- **Streamlit**: Interfaz gr√°fica para realizar predicciones desde la web.
- **Prometheus + Grafana**: Monitoreo del comportamiento de la API:
  - Latencia, uso de memoria, conteo de inferencias.
- **Locust**: Pruebas de carga para validar escalabilidad de la API.

> üß© Cada componente se despleg√≥ como contenedor independiente y se conect√≥ a trav√©s de redes virtuales internas. Las IPs asignadas por el cl√∫ster a cada servidor aseguran el enrutamiento correcto entre servicios.

---
##üõ†Ô∏è Tecnolog√≠as y Componentes Utilizados

El proyecto se compone de varios microservicios, cada uno desplegado en contenedores independientes, comunicados entre s√≠ dentro de un entorno orquestado con Kubernetes:

MLflow: Gesti√≥n de experimentos y modelos. Conectado a MinIO (artefactos) y MySQL (metadatos).

Airflow: Orquestaci√≥n de pipelines de preprocesamiento y entrenamiento.

MinIO: Almacenamiento local de artefactos, compatible con S3.

MySQL: Bases de datos para RawData, CleanData y metadata de MLflow y Airflow.

JupyterLab: Ejecuci√≥n de notebooks para carga, validaci√≥n y experimentaci√≥n.

FastAPI: API de inferencia del modelo en producci√≥n.

Streamlit: Interfaz gr√°fica para predicciones del modelo.

Prometheus + Grafana: Observabilidad y monitoreo de m√©tricas de inferencia.

Locust: Pruebas de carga para evaluar el rendimiento de la API.
