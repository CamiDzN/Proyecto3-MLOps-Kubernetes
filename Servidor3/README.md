# ğŸš€ Servicios en Servidor3

## ğŸ“‹ DescripciÃ³n General

Este README describe en detalle cÃ³mo estÃ¡n organizados y desplegados los servicios en Servidor3 dentro del proyecto PROYECTO3-MLOPS-KUBERNETES.

## ğŸ—ï¸ Arquitectura

El proyecto estÃ¡ desplegado a travÃ©s de Kubernetes con los siguientes componentes:

### ğŸ”¹ API de Inferencia (FastAPI)

Servicio que expone el modelo de machine learning para realizar predicciones en tiempo real.

- **TecnologÃ­a**: FastAPI
- **Funcionalidad**: Realiza inferencia utilizando el mejor modelo clasificado en producciÃ³n
- **MÃ©tricas**: IntegraciÃ³n con Prometheus para monitorizaciÃ³n
- **Endpoints**:
  - `/predict`: Realiza predicciones basadas en los datos de entrada
  - `/health`: Verifica el estado del servicio
  - `/metrics`: Expone mÃ©tricas para Prometheus

### ğŸ”¹ Interfaz de Usuario (Streamlit)

AplicaciÃ³n web que permite a los usuarios interactuar con el modelo de forma intuitiva.

- **TecnologÃ­a**: Streamlit
- **Funcionalidad**: Proporciona una interfaz grÃ¡fica para introducir datos y visualizar predicciones
- **IntegraciÃ³n**: Se comunica con la API de FastAPI para realizar predicciones

### ğŸ”¹ Pruebas de Carga (Locust)

Herramienta para realizar pruebas de rendimiento sobre la API de inferencia.

- **TecnologÃ­a**: Locust
- **Funcionalidad**: Simula mÃºltiples usuarios concurrentes para evaluar el rendimiento y la escalabilidad
- **Arquitectura**: ImplementaciÃ³n master-worker para distribuir la carga


### ğŸ”¹ Observabilidad (Prometheus + Grafana)

Stack de monitorizaciÃ³n para recolectar y visualizar mÃ©tricas de rendimiento.

- **Prometheus**: Recolecta mÃ©tricas de la API de inferencia
- **Grafana**: Visualiza las mÃ©tricas recolectadas en dashboards personalizables

**CaracterÃ­sticas de Prometheus:**
- RecolecciÃ³n de mÃ©tricas mediante scraping HTTP
- Almacenamiento eficiente de series temporales
- Lenguaje de consulta potente (PromQL)
- Alertas configurables
- IntegraciÃ³n con mÃºltiples exporters

**CaracterÃ­sticas de Grafana:**
- Dashboards personalizables y reutilizables
- Soporte para mÃºltiples fuentes de datos
- Visualizaciones avanzadas (grÃ¡ficos, tablas, heatmaps)
- Anotaciones y alertas
- ComparticiÃ³n y exportaciÃ³n de dashboards

## ğŸ› ï¸ Despliegue

### Requisitos Previos

- Kubernetes (MicroK8s)
- Docker
- Registro de imÃ¡genes local (puerto 32000)
- MLflow (para gestiÃ³n del modelo)

### Pasos de Despliegue

#### 1. Crear Namespace

```bash
sudo microk8s kubectl create namespace loadtest
```

#### 2. API de Inferencia (FastAPI)

```bash
# Situarse en el directorio de la API
cd api

# Construir y subir la imagen
docker build -t localhost:32000/fastapi-service:latest .
docker push localhost:32000/fastapi-service:latest

# Desplegar en Kubernetes
sudo microk8s kubectl -n loadtest apply -f k8s/fastapi-deployment.yaml
```

#### 3. Interfaz de Usuario (Streamlit)

```bash
# Situarse en el directorio de Streamlit
cd streamlit

# Construir y subir la imagen
docker build -t localhost:32000/streamlit-ui:latest .
docker push localhost:32000/streamlit-ui:latest

# Desplegar en Kubernetes
sudo microk8s kubectl -n loadtest apply -f k8s/streamlit-deployment.yaml
```

#### 4. Pruebas de Carga (Locust)

```bash
# Situarse en el directorio de Locust
cd locust

# Construir y subir la imagen
docker build -t localhost:32000/locust:latest .
docker push localhost:32000/locust:latest

# Desplegar en Kubernetes
sudo microk8s kubectl -n loadtest apply -f k8s/locust-k8s.yaml
```

#### 5. Observabilidad (Prometheus + Grafana)

```bash
# Situarse en el directorio de observabilidad
cd observability/k8s

# Crear el namespace y desplegar
sudo microk8s kubectl apply -f observability.yaml
```

## ğŸ” VerificaciÃ³n del Despliegue

Verificar que todos los servicios estÃ©n correctamente desplegados:

```bash
# Verificar pods
sudo microk8s kubectl -n loadtest get pods
sudo microk8s kubectl -n observability get pods

# Verificar servicios
sudo microk8s kubectl -n loadtest get svc
sudo microk8s kubectl -n observability get svc
```

## ğŸŒ Acceso a los Servicios

- **API de FastAPI**: http://[IP-DEL-NODO]:30080
  - DocumentaciÃ³n interactiva: http://[IP-DEL-NODO]:30080/docs
  - MÃ©tricas: http://[IP-DEL-NODO]:30080/metrics

  ![Despliegue de Api](public/Api.png)


- **Interfaz Streamlit**: http://[IP-DEL-NODO]:30081
  - Interfaz principal para usuarios finales
  - No requiere conocimientos tÃ©cnicos para su uso

  ![Visualizacion de Streamlit](public/Streamlit.png)


- **Locust**: http://[IP-DEL-NODO]:30009
  - Interfaz de configuraciÃ³n de pruebas
  - VisualizaciÃ³n de resultados en tiempo real
  - ExportaciÃ³n de informes

  ![Grafico de Locust](public/Locust.png)

- **Prometheus**: http://[IP-DEL-NODO]:30090
  - Explorador de mÃ©tricas
  - ConfiguraciÃ³n de alertas
  - Consultas PromQL

  ![Contador de consultas](public/Prometheus.png)


- **Grafana**: http://[IP-DEL-NODO]:30030
  - Credenciales por defecto: admin/admin
  - Dashboards preconfigurados
  - PersonalizaciÃ³n de visualizaciones

![Dashboard de Grafana](public/Grafana.png)

## ğŸ“ Estructura del Proyecto

```
.
â”œâ”€â”€ api/                    # Servicio de API con FastAPI
â”‚   â”œâ”€â”€ app/                # CÃ³digo de la aplicaciÃ³n
â”‚   â”‚   â”œâ”€â”€ main.py         # Punto de entrada de la API
â”‚   â”‚   â””â”€â”€ requirements.txt # Dependencias
â”‚   â”œâ”€â”€ Dockerfile          # ConfiguraciÃ³n para la imagen Docker
â”‚   â””â”€â”€ k8s/                # Manifiestos de Kubernetes
â”‚       â””â”€â”€ fastapi-deployment.yaml
â”œâ”€â”€ streamlit/              # Interfaz de usuario con Streamlit
â”‚   â”œâ”€â”€ app/                # CÃ³digo de la aplicaciÃ³n
â”‚   â”‚   â”œâ”€â”€ main.py         # Punto de entrada de la UI
â”‚   â”‚   â””â”€â”€ requirements.txt # Dependencias
â”‚   â”œâ”€â”€ Dockerfile          # ConfiguraciÃ³n para la imagen Docker
â”‚   â””â”€â”€ k8s/                # Manifiestos de Kubernetes
â”‚       â””â”€â”€ streamlit-deployment.yaml
â”œâ”€â”€ locust/                 # Pruebas de carga con Locust
â”‚   â”œâ”€â”€ locustfile.py       # ConfiguraciÃ³n de pruebas
â”‚   â”œâ”€â”€ Dockerfile          # ConfiguraciÃ³n para la imagen Docker
â”‚   â””â”€â”€ k8s/                # Manifiestos de Kubernetes
â”‚       â””â”€â”€ locust-k8s.yaml
â””â”€â”€ observability/          # MonitorizaciÃ³n con Prometheus y Grafana
    â”œâ”€â”€ prometheus.yml      # ConfiguraciÃ³n de Prometheus
    â””â”€â”€ k8s/                # Manifiestos de Kubernetes
        â”œâ”€â”€ grafana-datasources.yaml
        â””â”€â”€ observability.yaml
```

## colocar el titulooooooooooooooooo

# Pruebas de Carga e Inferencia

## Locust Prueba 1  
**NÃºmero de usuarios:** 300  
**Spawn rate:** 20 u/s  

  ![Grafico de Locust](public/1.1.png)
  ![Grafico de Locust](public/1.2.png)



En la secciÃ³n marcada en rojo de la grÃ¡fica de Locust vemos que, una vez alcanzados los 300 usuarios y el spawn rate de 20 u/s, el sistema se estabiliza en alrededor de **48â€“70 peticiones por segundo** con **cero fallos**, lo que indica que tu API mantiene ese nivel de throughput sin rechazar peticiones. A la par, el recuadro rojo de Grafana muestra que, durante ese mismo intervalo, la **latencia interna de inferencia** se mantiene en unos **0.024â€“0.025 s**, la **memoria virtual del proceso** ronda estable entre **2.9â€“3.2 GB**, y el contador de **inference_requests_total** crece de forma lineal (de unos **205 000 a 220 000**), confirmando que el endpoint atendiÃ³ cada llamada sin perder ninguna y con un uso de recursos constante.

---

## Locust Prueba 2  
**NÃºmero de usuarios:** 500  
**Spawn rate:** 40 u/s  

  ![Grafico de Locust](public/2.1.png)
  ![Grafico de Locust](public/2.2.png)


En la secciÃ³n resaltada en rojo de la grÃ¡fica de Locust observamos que, al llegar a 500 usuarios y un spawn rate de 40 u/s, el **RPS** oscila entre **40 y 80 peticiones por segundo** sin registrar fallos, lo cual demuestra que la API soporta ese nivel de concurrencia sin rechazar ninguna solicitud. SimultÃ¡neamente, en el recuadro rojo de Grafana vemos que la **latencia promedio de inferencia** asciende ligeramente de **0.0258 s** hasta **0.0262 s**, indicando un ligero incremento bajo carga; la **memoria virtual del proceso** sube momentÃ¡neamente de **3.08 GB** a **3.18 GB** justo al pico de carga, y el contador de **inference_requests_total** crece linealmente de **226 000 a 239 000** peticiones durante el intervalo, confirmando que todas las llamadas fueron procesadas y contabilizadas correctamente.

---

## Locust Prueba 3  
**NÃºmero de usuarios:** 800  
**Spawn rate:** 60 u/s  

  ![Grafico de Locust](public/3.1.png)
  ![Grafico de Locust](public/3.2.png)


En este tercer experimento, al subir la carga a 800 usuarios con un spawn rate de 60 u/s, podemos ver que Locust sigue generando trÃ¡fico sin registrar fallos, pero justo en el recuadro rojo de Grafana ya no aparecen datos y en Prometheus el endpoint de FastAPI aparece marcado como **DOWN** (context deadline exceeded). Eso sucede porque, al elevar tanto la concurrencia, el tiempo que tarda cada scrape de `/metrics` supera el timeout por defecto de Prometheus, de modo que sus peticiones GET se interrumpen antes de completar la descarga de mÃ©tricas. Para corregirlo, bastarÃ­a con ajustar en la configuraciÃ³n de Prometheus el `scrape_timeout` a un valor mayor (por ejemplo, igualar o acercarlo al `scrape_interval`), y/o reducir su `scrape_interval`, de forma que no lance una nueva peticiÃ³n antes de terminar la anterior. De ese modo, Prometheus podrÃ¡ volver a â€œrasparâ€ el endpoint `/metrics` con Ã©xito y Grafana retomarÃ¡ la visualizaciÃ³n de los datos.

---

## Conclusiones

Bajo cargas de hasta **500 usuarios**, la API mantiene un throughput estable (â‰ˆ 40â€“80 RPS), con latencias de inferencia de **0.024â€“0.026 s** y un uso de memoria suave (â‰ˆ 3 GB), atendiendo todas las peticiones sin fallos. Al subir a **800 usuarios**, Prometheus deja de raspar mÃ©tricas por timeout; ajustando su `scrape_timeout` o `scrape_interval`, y, de ser necesario, escalando mÃ¡s rÃ©plicas, la plataforma seguirÃ¡ siendo fiable y observable incluso ante picos muy altos.


## ğŸ”„ Flujo de Trabajo

1. **PreparaciÃ³n de datos**: Los datos del paciente se introducen a travÃ©s de la interfaz de Streamlit o se envÃ­an directamente a la API.

2. **Procesamiento de la solicitud**: 
   - La interfaz de Streamlit valida los datos y los envÃ­a a la API de FastAPI.
   - La API valida nuevamente los datos utilizando Pydantic.

3. **Inferencia del modelo**:
   - La API carga el modelo desde MLflow Registry.
   - Se realiza la predicciÃ³n utilizando los datos procesados.
   - Se registran mÃ©tricas de rendimiento en Prometheus.

4. **Respuesta al usuario**:
   - El resultado de la predicciÃ³n se devuelve a la interfaz de Streamlit.
   - Se presenta al usuario de forma clara y comprensible.

5. **MonitorizaciÃ³n continua**:
   - Prometheus recolecta mÃ©tricas de rendimiento de la API.
   - Grafana visualiza estas mÃ©tricas en dashboards personalizados.
   - Se generan alertas en caso de anomalÃ­as.

6. **Pruebas de carga**:
   - Locust permite realizar pruebas de rendimiento programadas o bajo demanda.
   - Los resultados ayudan a optimizar la configuraciÃ³n y el escalado.


## ğŸ‘¥ Contribuciones

Para contribuir al proyecto:

1. Haz un fork del repositorio
2. Crea una rama para tu funcionalidad (`git checkout -b feature/nueva-funcionalidad`)
3. Realiza tus cambios y haz commit (`git commit -am 'AÃ±adir nueva funcionalidad'`)
4. Sube los cambios a tu fork (`git push origin feature/nueva-funcionalidad`)
5. Crea un Pull Request

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT - ver el archivo [LICENSE](LICENSE) para mÃ¡s detalles.

## ğŸ‘¤ Autor

Luis Fernandez
- ğŸ’¼ [LinkedIn](https://www.linkedin.com/in/luis-carlos-fernandez-vargas-64466768)
