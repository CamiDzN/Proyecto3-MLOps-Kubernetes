# ğŸ§  DescripciÃ³n General del Proyecto

Este proyecto implementa una soluciÃ³n completa de MLOps distribuida en tres servidores, diseÃ±ada para gestionar todo el ciclo de vida de un modelo de machine learning que predice la probabilidad de readmisiÃ³n hospitalaria de pacientes con diabetes en un periodo de 30 dÃ­as.

La arquitectura del proyecto estÃ¡ basada en contenedores Docker orquestados con Kubernetes (MicroK8s) y estÃ¡ organizada en tres entornos funcionales independientes, desplegados en mÃ¡quinas virtuales diferentes:

- **Servidor 1**: Encargado del preprocesamiento automÃ¡tico de datos con Apache Airflow.
- **Servidor 2**: Responsable del registro de experimentos y gestiÃ³n de artefactos con MLflow y MinIO.
- **Servidor 3**: Despliega el modelo en producciÃ³n mediante una API con FastAPI, integra monitoreo con Prometheus & Grafana, pruebas de carga con Locust y una interfaz de usuario con Streamlit.

Este enfoque modular permite escalar y mantener cada componente de forma independiente, emulando un entorno real de producciÃ³n distribuido.

> ğŸ’¡ El objetivo principal es optimizar la gestiÃ³n hospitalaria mediante un sistema predictivo que permite anticipar la readmisiÃ³n de pacientes, mejorando asÃ­ la eficiencia de los recursos mÃ©dicos disponibles.

---

## ğŸ—‚ï¸ DistribuciÃ³n del Proyecto por Servidores

Este proyecto fue desarrollado colaborativamente y distribuido en tres mÃ¡quinas virtuales, cada una encargada de un componente clave del flujo de trabajo MLOps. Cada servidor tiene su propio `README.md` con detalles tÃ©cnicos y operativos especÃ­ficos:

| Servidor | Rol Principal                                   | Enlace al Detalle |
|----------|--------------------------------------------------|-------------------|
| ğŸŸ¦ Servidor 1 | Preprocesamiento de datos con Airflow           | [Ver README Servidor 1](./Servidor1/README.md) |
| ğŸŸ© Servidor 2 | Seguimiento de experimentos con MLflow y MinIO  | [Ver README Servidor 2](./Servidor2/README.md) |
| ğŸŸ¥ Servidor 3 | Despliegue, monitoreo y pruebas de inferencia   | [Ver README Servidor 3](./Servidor3/README.md) |

Cada una de estas secciones incluye:
- Los contenedores desplegados.
- Los DAGs y notebooks asociados.
- Instrucciones de uso y pruebas.

> ğŸ“Œ **Nota:** Todos los servidores estÃ¡n conectados en red local y comparten el acceso a la base de datos y el almacenamiento distribuido configurado para simular un entorno de producciÃ³n real.

---

---

## ğŸ§± Arquitectura General del Proyecto

El proyecto estÃ¡ distribuido en **tres servidores (mÃ¡quinas virtuales)** que trabajan de manera coordinada para implementar un pipeline completo de MLOps. Cada servidor aloja componentes especÃ­ficos de la arquitectura, asegurando modularidad, escalabilidad y claridad en la implementaciÃ³n.

A continuaciÃ³n se presenta el diagrama de la arquitectura general:

![Arquitectura del Proyecto](![image](https://github.com/user-attachments/assets/ab94263f-fc2e-488d-b24f-562a5c87e984)) 

### ğŸ”¹ Servidor 1 â€“ Preprocesamiento y Almacenamiento de Datos
- **Airflow**: OrquestaciÃ³n de pipelines de preprocesamiento y entrenamiento.
- **Base de Datos MySQL**: Almacena datos en dos capas:
  - `RawData`: Datos crudos separados en train, validation y test.
  - `CleanData`: Datos preprocesados listos para entrenamiento.
- **JupyterLab**: Desarrollo exploratorio y carga de datos desde notebooks.
- **DAGs**:
  - `preprocess_incremental`: Preprocesamiento automÃ¡tico.
  - `train_and_register`: Entrenamiento y selecciÃ³n del mejor modelo.

### ğŸ”¸ Servidor 2 â€“ Seguimiento de Experimentos
- **MLflow Tracking Server**: Registro de mÃ©tricas, parÃ¡metros y artefactos.
- **MinIO**: Almacenamiento compatible con S3 para guardar artefactos de modelos.
- **MySQL Metadata**: Almacena la metadata generada por MLflow.
- Imagen personalizada de MLflow desplegada con dependencias para conectividad segura.

### ğŸ”º Servidor 3 â€“ Despliegue, Observabilidad y Experiencia de Usuario
- **FastAPI**: API de inferencia conectada al modelo en producciÃ³n desde MLflow.
- **Streamlit**: Interfaz grÃ¡fica para realizar predicciones desde la web.
- **Prometheus + Grafana**: Monitoreo del comportamiento de la API:
  - Latencia, uso de memoria, conteo de inferencias.
- **Locust**: Pruebas de carga para validar escalabilidad de la API.

> ğŸ§© Cada componente se desplegÃ³ como contenedor independiente y se conectÃ³ a travÃ©s de redes virtuales internas. Las IPs asignadas por el clÃºster a cada servidor aseguran el enrutamiento correcto entre servicios.

---
## ğŸ› ï¸ TecnologÃ­as y Componentes Utilizados

El proyecto se compone de varios microservicios, cada uno desplegado en contenedores independientes, comunicados entre sÃ­ dentro de un entorno orquestado con Kubernetes:

MLflow: GestiÃ³n de experimentos y modelos. Conectado a MinIO (artefactos) y MySQL (metadatos).

Airflow: OrquestaciÃ³n de pipelines de preprocesamiento y entrenamiento.

MinIO: Almacenamiento local de artefactos, compatible con S3.

MySQL: Bases de datos para RawData, CleanData y metadata de MLflow y Airflow.

JupyterLab: EjecuciÃ³n de notebooks para carga, validaciÃ³n y experimentaciÃ³n.

FastAPI: API de inferencia del modelo en producciÃ³n.

Streamlit: Interfaz grÃ¡fica para predicciones del modelo.

Prometheus + Grafana: Observabilidad y monitoreo de mÃ©tricas de inferencia.

Locust: Pruebas de carga para evaluar el rendimiento de la API.

## ğŸš€ Â¿CÃ³mo ejecutar el proyecto completo?
âœ… AsegÃºrate de que los 3 servidores estÃ©n activos, conectados en la misma red y con Kubernetes (MicroK8s) habilitado.

ğŸ”Œ Paso a paso por servidor
ğŸ–¥ï¸ Servidor 1 â€” Preprocesamiento y orquestaciÃ³n

Despliega los servicios con:

```bash
kubectl apply -f Servidor1/kubernetes/
```
Accede a Airflow y ejecuta el DAG preprocess_incremental.

ğŸ—ƒï¸ Servidor 2 â€” Almacenamiento y MLflow

Construye y publica la imagen personalizada de MLflow:

```bash
docker build -t custom-mlflow:latest .
docker tag custom-mlflow:latest localhost:32000/custom-mlflow:latest
docker push localhost:32000/custom-mlflow:latest
```
Despliega los servicios:

```bash
kubectl apply -f Servidor2/kubernetes/
```
Ejecuta el job para crear el bucket en MinIO:

```bash
kubectl apply -f Servidor2/kubernetes/create-minio-bucket.yaml
```
ğŸ“¡ Servidor 3 â€” Inferencia, monitoreo y UI


Despliega los servicios con:

```bash
kubectl apply -f Servidor3/kubernetes/
```
Accede a la API o interfaz de Streamlit para hacer predicciones.

Verifica mÃ©tricas en Prometheus y visualÃ­zalas en Grafana.

Ejecuta pruebas de carga con Locust.

## ğŸ“ Estructura del Proyecto
El repositorio se organiza en tres carpetas principales, cada una correspondiente a uno de los servidores utilizados en el despliegue distribuido del sistema MLOps. A continuaciÃ³n, se detalla el contenido de cada uno:

ğŸŸ¢ Servidor1/
Responsable del procesamiento de datos y entrenamiento de modelos.

```bash
Servidor1/
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ preprocess_diabetes_data.py
â”‚   â””â”€â”€ training_models_diabetes_data.py
â”œâ”€â”€ jupyterlab-Image/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ kubernetes/
â”‚   â”œâ”€â”€ deployments/
â”‚   â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ jupyter-deployment.yaml
â”‚   â””â”€â”€ mysql-deployment.yaml
â”œâ”€â”€ Cargar RawData.ipynb
â”œâ”€â”€ Verificacion_Preprocesamiento.ipynb
â”œâ”€â”€ Experimentos.ipynb
â””â”€â”€ docker-compose.yaml
```

ğŸ”µ Servidor2/
Encargado del almacenamiento de artefactos y seguimiento de experimentos.

```bash
Servidor2/
â”œâ”€â”€ kubernetes/
â”‚   â”œâ”€â”€ create-minio-bucket.yaml
â”‚   â”œâ”€â”€ minio-deployment.yaml
â”‚   â”œâ”€â”€ mlflow-deployment.yaml
â”‚   â”œâ”€â”€ mysql-deployment.yaml
â”‚   â””â”€â”€ servicios.yaml
â”œâ”€â”€ Dockerfile
â””â”€â”€ README.md
```
ğŸŸ£ Servidor3/
Contiene la API de inferencia, observabilidad y la interfaz grÃ¡fica.

```bash
Servidor3/
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ app/
â”‚   â””â”€â”€ k8s/
â”‚       â””â”€â”€ Dockerfile
â”œâ”€â”€ locust/
â”‚   â”œâ”€â”€ locustfile.py
â”‚   â””â”€â”€ k8s/
â”œâ”€â”€ observability/
â”‚   â”œâ”€â”€ k8s/
â”‚   â”‚   â”œâ”€â”€ grafana-datasources.yaml
â”‚   â”‚   â”œâ”€â”€ observability.yaml
â”‚   â””â”€â”€ prometheus.yml
â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ Api.png
â”‚   â”œâ”€â”€ Grafana.png
â”‚   â”œâ”€â”€ Prometheus.png
â”‚   â”œâ”€â”€ Locust.png
â”‚   â””â”€â”€ Streamlit.png
â”œâ”€â”€ streamlit/
â”‚   â”œâ”€â”€ app/
â”‚   â””â”€â”€ k8s/
â””â”€â”€ README.md
```

## ğŸ”„ Flujo de Trabajo del Proyecto

1. **Carga inicial del dataset (`Jupyter - Cargar RawData.ipynb`)**
   - Limpieza opcional de las bases de datos `RawData` y `CleanData`.
   - Descarga del CSV original y divisiÃ³n en `train_pool`, `validation_data` y `test_data`.
   - Carga incremental de `train_data` en lotes de 15.000 registros para simular flujo real.

2. **Preprocesamiento de datos (`Airflow - preprocess_incremental.py`)**
   - `load_splits`: carga los tres conjuntos desde `RawData`.
   - `clean_splits`: crea la variable `early_readmit`, aplica limpieza y codificaciÃ³n.
   - `select_features`: selecciona las 50 mejores caracterÃ­sticas.
   - `save_processed`: guarda los nuevos conjuntos en la base `CleanData`.

3. **ValidaciÃ³n de tablas (`Jupyter - VerificaciÃ³n Preprocesamiento.ipynb`)**
   - ConfirmaciÃ³n manual de las tablas generadas en `CleanData`.

4. **ExperimentaciÃ³n de modelos (`Jupyter - Experimentos.ipynb`)**
   - Entrenamiento de distintos modelos (`LR`, `RF`) y registro en MLflow.
   - EvaluaciÃ³n con mÃ©tricas de validaciÃ³n y test.
   - PromociÃ³n del mejor modelo a producciÃ³n bajo el nombre `best_diabetes_readmission_model`.

5. **Entrenamiento automatizado (`Airflow - train_and_register.py`)**
   - `train_models`: entrena todos los modelos y registra resultados.
   - `select_and_promote`: promueve automÃ¡ticamente el mejor modelo a producciÃ³n.

6. **API de Inferencia (`FastAPI`)**
   - Consulta el modelo en producciÃ³n desde MLflow y MinIO.
   - Expone endpoints de predicciÃ³n y monitoreo (`/metrics`).

7. **Monitoreo de rendimiento (`Prometheus + Grafana`)**
   - RecolecciÃ³n de mÃ©tricas como latencia, carga y uso de memoria del contenedor.
   - VisualizaciÃ³n de mÃ©tricas en tiempo real con Grafana.

8. **Pruebas de carga (`Locust`)**
   - SimulaciÃ³n de usuarios concurrentes para evaluar la estabilidad del API.

9. **Interfaz de usuario (`Streamlit`)**
   - Permite ingresar datos de pacientes para obtener predicciones de readmisiÃ³n.
   - Mejora la capacidad de gestiÃ³n hospitalaria al anticipar posibles reingresos.
